{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mutual Information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mi_scores(X, y):\n",
    "    X = X.copy()\n",
    "    # Label encoding for categoricals\n",
    "    for colname in X.select_dtypes([\"object\", \"category\"]):\n",
    "        X[colname], _ = X[colname].factorize()\n",
    "    # All discrete features should now have integer dtypes\n",
    "    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n",
    "    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    return mi_scores\n",
    "\n",
    "# All discrete features should now have integer dtypes (double-check this before using MI!)\n",
    "discrete_features = X.dtypes == int\n",
    "\n",
    "def plot_mi_scores(scores):\n",
    "    scores = scores.sort_values(ascending=True)\n",
    "    width = np.arange(len(scores))\n",
    "    ticks = list(scores.index)\n",
    "    plt.barh(width, scores)\n",
    "    plt.yticks(width, ticks)\n",
    "    plt.title(\"Mutual Information Scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating New Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer[[\"Type\", \"Level\"]] = (  # Create two new features\n",
    "    customer[\"Policy\"]           # from the Policy feature\n",
    "    .str                         # through the string accessor\n",
    "    .split(\" \", expand=True)     # by splitting on \" \"\n",
    "                                 # and expanding the result into separate columns\n",
    ")\n",
    "\n",
    "customer[[\"Policy\", \"Type\", \"Level\"]].head(10)\n",
    "\n",
    "autos[\"make_and_style\"] = autos[\"make\"] + \"_\" + autos[\"body_style\"]\n",
    "autos[[\"make\", \"body_style\", \"make_and_style\"]].head()\n",
    "\n",
    "customer[\"AverageIncome\"] = (\n",
    "    customer.groupby(\"State\")  # for each state\n",
    "    [\"Income\"]                 # select the income\n",
    "    .transform(\"mean\")         # and compute its mean\n",
    ")\n",
    "\n",
    "customer[[\"State\", \"Income\", \"AverageIncome\"]].head(10)\n",
    "\n",
    "\n",
    "# Create splits\n",
    "df_train = customer.sample(frac=0.5)\n",
    "df_valid = customer.drop(df_train.index)\n",
    "\n",
    "# Create the average claim amount by coverage type, on the training set\n",
    "df_train[\"AverageClaim\"] = df_train.groupby(\"Coverage\")[\"ClaimAmount\"].transform(\"mean\")\n",
    "\n",
    "# Merge the values into the validation set\n",
    "df_valid = df_valid.merge(\n",
    "    df_train[[\"Coverage\", \"AverageClaim\"]].drop_duplicates(),\n",
    "    on=\"Coverage\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "df_valid[[\"Coverage\", \"AverageClaim\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the column to be split is named 'ColumnToSplit'\n",
    "# First split by comma\n",
    "split_by_comma = customer[\"ColumnToSplit\"].str.split(\",\", expand=True)\n",
    "\n",
    "# Then split the first part by period\n",
    "split_by_period = split_by_comma[0].str.split(\".\", expand=True)\n",
    "\n",
    "# Combine the results\n",
    "result = pd.concat([split_by_period, split_by_comma[1]], axis=1)\n",
    "result.columns = [\"Part1\", \"Part2\", \"Part3\"]\n",
    "\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clustering K-Means**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Define a list of the features to be used for the clustering\n",
    "features = ['LotArea', 'TotalBsmtSF', 'FirstFlrSF', 'SecondFlrSF','GrLivArea']\n",
    "\n",
    "\n",
    "# Standardize\n",
    "X_scaled = X.loc[:, features]\n",
    "X_scaled = (X_scaled - X_scaled.mean(axis=0)) / X_scaled.std(axis=0)\n",
    "\n",
    "\n",
    "# YOUR CODE HERE: Fit the KMeans model to X_scaled and create the cluster labels\n",
    "kmeans = KMeans(n_cluster = 10, random_state=0)\n",
    "X[\"Cluster\"] = kmeans.fit_predict(X)\n",
    "X[\"Cluster\"] = X[\"Cluster\"].astype(\"category\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PCA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pca(X, standardize=True):\n",
    "    # Standardize\n",
    "    if standardize:\n",
    "        X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "    # Create principal components\n",
    "    pca = PCA()\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    # Convert to dataframe\n",
    "    component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n",
    "    X_pca = pd.DataFrame(X_pca, columns=component_names)\n",
    "    # Create loadings\n",
    "    loadings = pd.DataFrame(\n",
    "        pca.components_.T,  # transpose the matrix of loadings\n",
    "        columns=component_names,  # so the columns are the principal components\n",
    "        index=X.columns,  # and the rows are the original features\n",
    "    )\n",
    "    return pca, X_pca, loadings\n",
    "\n",
    "\n",
    "def plot_variance(pca, width=8, dpi=100):\n",
    "    # Create figure\n",
    "    fig, axs = plt.subplots(1, 2)\n",
    "    n = pca.n_components_\n",
    "    grid = np.arange(1, n + 1)\n",
    "    # Explained variance\n",
    "    evr = pca.explained_variance_ratio_\n",
    "    axs[0].bar(grid, evr)\n",
    "    axs[0].set(\n",
    "        xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)\n",
    "    )\n",
    "    # Cumulative Variance\n",
    "    cv = np.cumsum(evr)\n",
    "    axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n",
    "    axs[1].set(\n",
    "        xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n",
    "    )\n",
    "    # Set up figure\n",
    "    fig.set(figwidth=8, dpi=100)\n",
    "    return axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"highway_mpg\", \"engine_size\", \"horsepower\", \"curb_weight\"]\n",
    "\n",
    "X = df.copy()\n",
    "y = X.pop('price')\n",
    "X = X.loc[:, features]\n",
    "\n",
    "# Standardize\n",
    "X_scaled = (X - X.mean(axis=0)) / X.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create principal components\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Convert to dataframe\n",
    "component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n",
    "X_pca = pd.DataFrame(X_pca, columns=component_names)\n",
    "\n",
    "X_pca.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Target Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding split\n",
    "X_encode = df.sample(frac=0.20, random_state=0)\n",
    "y_encode = X_encode.pop(\"SalePrice\")\n",
    "\n",
    "# Training split\n",
    "X_pretrain = df.drop(X_encode.index)\n",
    "y_train = X_pretrain.pop(\"SalePrice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VRM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
