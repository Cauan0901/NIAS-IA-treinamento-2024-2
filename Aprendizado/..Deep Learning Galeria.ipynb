{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Basic Model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras   # this is how we import the Keras library\n",
    "from tensorflow.keras import layers     # layers are the basic building blocks of a neural network\n",
    "\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    min_delta=0.001, # minimium amount of change to count as an improvement\n",
    "    patience=20, # how many epochs to wait before stopping\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "model = keras.Sequential([    # a Sequential model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor\n",
    "    # the hidden ReLU layers\n",
    "    layers.Dense(units=4, activation='relu', input_shape=[2]), \n",
    "    layers.Dense(units=3, activation='relu'),\n",
    "    # the linear output layer \n",
    "    layers.Dense(units=1), # Output layer\n",
    "])\n",
    "\n",
    "model.compile(   # compile the model\n",
    "    optimizer='adam',  # the optimizer is the algorithm that adjusts the weights of the neurons during\n",
    "    loss='mae', # the loss function is a measure of how well the model is performing\n",
    ")\n",
    "\n",
    "train = model.fit(\n",
    "    X_train, y_train, # the training data\n",
    "    validation_data=(X_valid, y_valid), # the validation_data argument is used to provide a separate set of data to test the model\n",
    "    batch_size=256, # the batch size is the number of training examples used in one iteration of the optimizer\n",
    "    epochs=10, # the number of epochs is the number of times the model will cycle through the data\n",
    "    callbacks=[early_stopping], # put your callbacks in a list\n",
    "    verbose=0,  # turn off training log\n",
    ")\n",
    "# We've told Keras to feed the optimizer 256 rows of the training data at a time (the batch_size) \n",
    "# and to do that 10 times all the way through the dataset (the epochs).\n",
    "\n",
    "# convert the training history to a dataframe\n",
    "train_df = pd.DataFrame(history.history)\n",
    "# use Pandas native plot method\n",
    "train_df['loss'].plot();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Dropout e Batch Normalization*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Adding a dropout layer\n",
    "keras.Sequential([\n",
    "    # ...\n",
    "    layers.Dropout(rate=0.3), # apply 30% dropout to the next layer\n",
    "    layers.Dense(16),\n",
    "    # ...\n",
    "])\n",
    "\n",
    "# Adding a batch normalization layer\n",
    "# ...\n",
    "layers.Dense(16, activation='relu'),\n",
    "layers.BatchNormalization(),\n",
    "#...\n",
    "#   or\n",
    "layers.Dense(16),\n",
    "layers.BatchNormalization(),\n",
    "layers.Activation('relu'),\n",
    "\n",
    "\n",
    "# Example\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(1024, activation='relu', input_shape=[11]),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(1),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Binary Classification*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(4, activation='relu', input_shape=[33]),\n",
    "    layers.Dense(4, activation='relu'),    \n",
    "    layers.Dense(1, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['binary_accuracy'],\n",
    ")\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    patience=10,\n",
    "    min_delta=0.001,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    batch_size=512,\n",
    "    epochs=1000,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=0, # hide the output because we have so many epochs\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VRM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
